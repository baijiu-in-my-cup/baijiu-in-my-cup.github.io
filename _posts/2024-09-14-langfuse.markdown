---
layout: post
title: "你的大模型应用表现真的好吗？借助 Dify + Langfuse 一探究竟"
subtitle:   "Is your large model application really performing well? Find out with Dify + Langfuse"
date:       2024-09-14 09:00:00
author:     "Bryan"
header-mask: 0.3
catalog:    true
tags:
    - dify
    - langfuse
    - agent
    - llm
---

## 背景介绍
众所周知，大模型应用的输出存在着一些不确定性，往往需要迭代多轮才能得到较为稳定的输出结果，因此开发者往往需要关注大模型应用的实际表现，并进行有针对性的优化。

然而常规 Web 服务的监控机制往往无法满足大模型应用的监控需求，因为大模型应用往往关注的不仅仅是响应延迟、吞吐量等基础指标，而是需要关注大模型应用输出的语义正确性。因此，本文将介绍如何借助 Dify 和 Langfuse 工具，展示如何为大模型应用必要监控，方便进行有定向优化。

## 基础介绍

#### Dify
[Dify](https://github.com/langgenius/dify) 是一款热门的开源 LLMOps 服务，作为一个大模型应用的基础设施，可以帮助开发者快速构建 LLM 应用。关于 Dify 的介绍之前整理过很多，比如 [大模型应用基础服务 Dify 深度解读](https://zhuanlan.zhihu.com/p/706381113), 感兴趣的可以去了解下，这边就不再深入展开了。

#### Langfuse

[Langfuse](https://github.com/langfuse/langfuse) 是一款开源的大模型应用监控服务，可以为大模型应用提供监控能力。比如面对 Dify 复杂的任务流，可以借助 Langfuse 跟踪各个基础环节输出的结果，方便深入定位问题。

除了常规的内容搜集与展示外，Langfuse 还有两个值得重点关注的能力：

1. LLM 评估，可以借助大模型评估现有业务输出的内容，充当 [LLM-as-a-Judge](https://arxiv.org/abs/2306.05685) 角色。考虑到大模型应用输出的都是大量的文本内容，常规的评估方式往往无法满足需求，因此借助 LLM 评估可以更精准快速地评估输出内容（这个功能目前 Langfuse 还处于实验阶段，私有化部署暂时还不支持，预计后续会很快上线）；
2. 数据集管理，数据集管理可以帮助我们通过批量标准手段快速测试大模型应用，在进行大模型应用优化时，可以借助数据集快速测试优化后的效果，避免负优化；

Dify 官方目前直接支持的监控方案为 LangSmith 和 Langfuse。 LangSmith 功能更强，但是需要付费，而且没有提供私有化部署的能力，因此 Langfuse 算是一个开源的经济方案。

## 部署与集成

#### 服务部署

Dify 和 Langfuse 都提供了多种部署方案，常规情况下，如果希望进行私有化部署，基于 docker compose 的方案是最省心的。

Dify 的部署流程可以参考 [Dify Docker Compose 部署](https://docs.dify.ai/v/zh-hans/getting-started/install-self-hosted/docker-compose)

Langfuse 的部署流程可以参考 [Langfuse Docker Compose 部署](https://langfuse.com/docs/deployment/local)

#### 服务集成

Dify 官方提供了对 Langfuse 的支持，因此可以在 Dify 应用中通过配置 Langfuse 地址，public key, secret key 信息，就可以快速集成 Langfuse 服务。

![langfuse](/img/in-post/langfuse/langfuse.png)

详细的服务集成的流程可以参考 [Dify 官方文档中 Langfuse 集成](https://docs.dify.ai/v/zh-hans/guides/monitoring/integrate-external-ops-tools/integrate-langfuse)

服务集成之后，在 Dify 应用中进行正常使用，就可以跟踪输出结果与应用中间环节的输出了，实际测试跟踪效果如下所示：

![usage](/img/in-post/langfuse/usage.png)

上图是一个 RAG 应用，可以看到 Langfuse 除了可以跟踪最终输出，右侧也可以看到中间知识检索的结果，方便深入定位各个环节中的问题。

## 数据集与自动化评估

在大模型应用的持续迭代中，为了验证迭代效果，往往会构建一个标准的自动化测试数据集 (Dataset)。基于标准的数据集与大模型自动化评估手段，可以快速直观看到优化后的效果差异。除此之外，Langfuse 也支持根据线上跟踪的情况动态新增数据集，比如可以将之前表现不佳的用户问题动态加入数据集，方便进行有针对性的优化：

![dataset](/img/in-post/langfuse/dataset-example-workflow.webp)

#### 数据集的构建

Langfuse 提供了 sdk 帮助快速构建数据集，构建数据集主要包含创建数据集，添加测试项。

创建测试集可以直接调用 `create_dataset` 方法：

```python
langfuse.create_dataset(
    name="<dataset_name>",
    description="My first dataset",
    metadata={
        "author": "Alice",
        "date": "2022-01-01",
        "type": "benchmark"
    }
)
```

向测试集中添加测试项可以调用 `create_dataset_item()` 方法：

```python
langfuse.create_dataset_item(
    dataset_name="<dataset_name>",
    input={
        "text": "hello world"
    },
    expected_output={
        "text": "hello world"
    },
    metadata={
        "model": "llama3",
    }
)
```

创建完成后就可以在 Langfuse 网页端实时查看了。

#### Dify 任务提交

为了进行批量自动化测试，需要获取数据集中所有的测试项，依次提交给 Dify 服务，并将本次任务与测试项进行绑定：

```python
# 调用 dify 接口发送消息，获取 dify 响应

async def send_chat_message(
    query: str,
    inputs: dict = {},
    url: str = os.getenv("DIFY_API_BASE", ""),
    api_key: str = os.getenv("DIFY_API_KEY", ""),
    response_mode: Literal["streaming", "blocking"] = "blocking",
    user: str = "auto_test",
    file_array: list = [],
):
    chat_url = f"{url}/chat-messages"
    headers = {"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"}
    payload = {
        "inputs": inputs,
        "query": query,
        "response_mode": response_mode,
        "conversation_id": "",
        "user": user,
        "files": file_array,
    }
    async with aiohttp.ClientSession() as session:
        async with session.post(chat_url, headers=headers, json=payload) as response:
            ret = await response.json()
            status = ret.get("status")
            message = ret.get("message")
            if status and message:
                raise ValueError(f"{status}: {message}")
            return ret

async def run_dataset_item(item, run_name):
    response = await send_chat_message(item.input)

    # 将 dify 返回的 message_id 与 langfuse 中的 trace id 进行关联

    item.link(
        trace_or_observation=None,
        run_name=run_name,
        trace_id=response["message_id"],
        observation_id=None,
    )
    return response
```

如果在 Dify 中绑定了 Langfuse 服务，那么 Dify 处理消息时，就会将完整的调用链路发送给 Langfuse。如果通过 `item.link()` 执行了数据集的绑定，那么实际就能看到批量测试结果，而且点击 trace 列可以跟踪单次跳转的详情。类似如下所示：

![trace](/img/in-post/langfuse/trace.png)

实际测试发现，Dify 的监控消息是异步发送给 Langfuse, 因此如果异步任务堆积，可能会导致 Langfuse 推送延迟。

#### 自动化评估

后续官方的私有化部署自动化评估发布之后，应该只需要配置大模型即可进行自动评估了。而目前阶段可以基于第三方框架进行评估，之后再上传至 Langfuse 中。常规 RAG 的评估框架之前的文章中 [从开发到部署，搭建离线私有大模型知识库](https://zhuanlan.zhihu.com/p/689947142), 可以选择 ragas, trulens 等。

计算评估得分后调用 Langfuse 中的 `score()` 方法，将得分传入 Langfuse：

```python
langfuse_generation = langfuse.generation(
    name="guess-countries",
    input=messages,
    output=openai_completion,
    model="gpt-3.5-turbo",
    start_time=generationStartTime,
    end_time=datetime.now()
)

# 自定义评估方法 evaluate_func，计算所需的评估得分

langfuse_generation.score(
    name="exact_match",
    value=evaluate_func(completion, item.expected_output)
)
```

个人实际选择 ragas 作为评估方法，评估完成后可以在网页端直接查看：

![after_test](/img/in-post/langfuse/after_test.png)


## 总结

本文是对 Dify + Langfuse 的初步实践，从个人使用体验来看，由于 Dify 官方提供了完整的支持，Langfuse 的接入变得相对简单，可以快速跟踪现有大模型应用的执行链路。

但是目前 Langfuse 还处于快速迭代中，自动化测评的集成还不是很完善，需要自己做必要的代码开发，才能真正满足全自动化测试的需求。但是整体而言，在 Dify 应用中，Langfuse 还是一个不错的选择，推荐大家尝试一下。
